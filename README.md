# ğŸ¤– AI Nexus - Multi-AI Chat Platform

> **Query multiple AI models simultaneously and get synthesized answers** - All in one place!

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)

---

## ğŸš€ Quick Start (3 Steps - 2 Minutes!)

### Step 1: Download

```bash
git clone https://github.com/jimmymannekkattu/Multi-LLM-Aggregator.git
cd Multi-LLM-Aggregator
```

### Step 2: Run

```bash
chmod +x run.sh
./run.sh
```

### Step 3: Start Chatting!

**Three Ways to Use AI Nexus:**

1. **ğŸ–¥ï¸ Desktop App** - Open browser: http://localhost:8501
2. **ğŸ’¬ Web Chat** - Open file: `examples/chat-full.html`  
3. **ğŸ“± Mobile App** - Scan QR code from desktop app

**That's it!** AI Nexus is now running with free models. No API keys needed to start!

---

## âœ¨ What is AI Nexus?

AI Nexus lets you:
- âœ… **Ask one question** â†’ Get answers from **multiple AIs** (GPT, Claude, Gemini, etc.)
- âœ… **Use FREE models** â†’ No API keys required (uses g4f)
- âœ… **Chat from anywhere** â†’ Desktop, Web, or Mobile
- âœ… **Get smarter answers** â†’ AI synthesizes multiple responses into one
- âœ… **Keep it private** â†’ Everything runs on your computer

---

## ğŸ“‹ Prerequisites

**Required:**
- âœ… **Python 3.10+** â†’ [Download here](https://www.python.org/downloads/)
- âœ… **Ollama** â†’ [Install here](https://ollama.com/) â† This runs the local "brain"

**Optional:**
- API keys for premium models (OpenAI, Anthropic, Google)
- Node.js (only for building mobile app from source)

### Installing Ollama

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Download a model (required for synthesis)
ollama pull llama3
```

---

## ğŸ’» Usage Guide

### Option 1: Web Chat Interface (Recommended for Beginners)

1. **Start the server**: `./run.sh`
2. **Open the chat**: Double-click `examples/chat-full.html`
3. **Start chatting!**

**Features:**
- âœ¨ Beautiful modern UI
- ğŸ¯ Select which AI models to use
- âš™ï¸ Configure API keys
- ğŸ’¾ Settings saved automatically
- ğŸ“± Mobile-responsive

### Option 2: Desktop App (Power Users)

1. **Start**: `./run.sh`
2. **Open**: http://localhost:8501
3. **Full features**: Discovery, Memory, Network Nodes

### Option 3: Mobile App

1. **Start backend**: Already running from `./run.sh`
2. **Open desktop app**: http://localhost:8501
3. **Go to Mobile tab**: Scan QR code with your phone
4. **Install Expo Go**: From App/Play Store
5. **Scan QR**: Connect instantly!

---

## ğŸ¯ Key Features

### 1. **Multi-AI Querying**
Ask one question, get answers from:
- ChatGPT (OpenAI)
- Claude (Anthropic)  
- Gemini (Google)
- Perplexity
- Free Web Models (g4f)
- Local Ollama Models

### 2. **WebSocket & Streaming API**
- Real-time WebSocket chat at `/ws/chat`
- Server-sent events at `/stream/chat`
- Full REST API at `/chat`
- Interactive docs at http://localhost:8000/docs

### 3. **Global Model Discovery**
Search and add models from:
- g4f (free web models)
- OpenRouter (100+ models)
- Local Ollama instances

### 4. **Smart Memory System**
- Learns from online AI responses
- Builds local knowledge base
- Makes offline models smarter over time
- Export training data for fine-tuning

### 5. **Network Nodes**
- Connect multiple computers
- Share Ollama models across network
- Distributed AI processing

---

## ğŸ”‘ Adding API Keys (Optional)

AI Nexus works **without API keys** using free models. For better performance:

**In Desktop App:**
1. Sidebar â†’ Expand provider (OpenAI, Anthropic, etc.)
2. Paste your API key
3. Toggle **ON**

**In Web Chat:**
1. Click **â˜° Menu** â†’ **Settings**
2. Enter your API keys
3. Click **ğŸ’¾ Save**

**Supported Providers:**
- OpenAI â†’ `sk-...`
- Anthropic â†’ `sk-ant-...`
- Google â†’ `AIza...`
- Perplexity â†’ `pplx-...`
- OpenRouter â†’ `sk-or-...`

---

## ğŸ§ª Testing

```bash
# Run all tests
pytest tests/

# Run specific test
pytest tests/test_api.py -v
```

**Test Coverage:**
- âœ… API endpoints (4 tests)
- âœ… Model discovery (6 tests)
- âœ… Provider integrations (4 tests)
- âœ… WebSocket connections (3 tests)

---

## ğŸ“¡ API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/health` | GET | Health check |
| `/models` | GET | List available models |
| `/chat` | POST | Standard chat request |
| `/ws/chat` | WS | WebSocket real-time chat |
| `/stream/chat` | POST | Streaming responses (SSE) |
| `/history` | GET | Chat history |
| `/docs` | GET | Interactive API documentation |

**Full API Documentation:** See [wiki/API-Documentation.md](wiki/API-Documentation.md)

---

## ğŸ³ Docker Deployment (Alternative)

```bash
# Build and run
docker-compose up --build

# Access
Desktop App: http://localhost:8501
API: http://localhost:8000
Ollama: http://localhost:11434
```

---

## ğŸ“ Project Structure

```
Multi-LLM-Aggregator/
â”œâ”€â”€ run.sh                  # One-click startup script
â”œâ”€â”€ app.py                  # Streamlit desktop app
â”œâ”€â”€ api.py                  # FastAPI backend
â”œâ”€â”€ llm_providers.py        # AI model integrations
â”œâ”€â”€ offline_model.py        # Synthesis engine
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ examples/               # Example clients
â”‚   â”œâ”€â”€ chat-full.html     # Full-featured web chat
â”‚   â”œâ”€â”€ websocket_client.py # Python WebSocket example
â”‚   â””â”€â”€ stream_client.py   # Python streaming example
â”œâ”€â”€ tests/                  # Test suite
â”œâ”€â”€ agents/                 # Core logic
â”‚   â”œâ”€â”€ discovery.py       # Model discovery
â”‚   â””â”€â”€ memory.py          # RAG memory system
â”œâ”€â”€ mobile/                 # React Native mobile app
â””â”€â”€ wiki/                   # Documentation
```

---

## ğŸ†˜ Troubleshooting

### "ModuleNotFoundError"
```bash
pip install -r requirements.txt
```

### "Ollama 404" or "Model Not Found"
```bash
ollama serve          # Start Ollama
ollama pull llama3   # Download model
```

### "Port already in use"
```bash
# Kill process on port 8501 or 8000
lsof -ti:8501 | xargs kill  # Mac/Linux
```

### Mobile app won't connect
1. Same Wi-Fi network?
2. Firewall blocking port 8000?
3. Try: `sudo ufw allow 8000/tcp` (Linux)

**See full troubleshooting:** [wiki/Troubleshooting.md](wiki/Troubleshooting.md)

---

## ğŸ“š Documentation

- **[User Guide](wiki/User-Guide.md)** - Complete feature walkthrough
- **[API Documentation](wiki/API-Documentation.md)** - Endpoint reference
- **[Mobile Setup](wiki/Mobile-App-Setup.md)** - Mobile app guide
- **[WebSocket Chat](WEBSOCKET_CHAT_GUIDE.md)** - Web chat interface
- **[Troubleshooting](wiki/Troubleshooting.md)** - Common issues
- **[Architecture](agents.md)** - Technical details

---

## ğŸ¯ Common Use Cases

**1. Quick Question â†’ One Answer**
- Select multiple models in web chat
- Ask your question
- Get synthesized answer in seconds

**2. Compare AI Perspectives**
- Enable multiple providers
- See how different AIs approach the problem
- Get comprehensive view

**3. Offline AI Assistant**
- Use only Ollama models
- No internet needed
- Complete privacy

**4. Build Your Own AI**
- Enable memory/learning
- Ask questions with online models
- Export training data
- Fine-tune your own model

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repo
2. Create a feature branch
3. Submit a pull request

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE) file

---

## ğŸ™ Acknowledgments

- [g4f](https://github.com/xtekky/gpt4free) - Free AI access
- [Ollama](https://ollama.com/) - Local model runtime
- [Streamlit](https://streamlit.io/) - Beautiful UI framework
- [FastAPI](https://fastapi.tiangolo.com/) - Modern API framework

---

## ğŸ’¡ Need Help?

- ğŸ“– **Check the Wiki**: [GitHub Wiki](https://github.com/jimmymannekkattu/Multi-LLM-Aggregator/wiki)
- ğŸ› **Report Issues**: [GitHub Issues](https://github.com/jimmymannekkattu/Multi-LLM-Aggregator/issues)
- ğŸ’¬ **Questions**: Open a discussion

---

**Made with â¤ï¸ for the AI community**
